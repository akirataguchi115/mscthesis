\chapter{Discussion\label{discussion}}

\textcolor{red}{indications}

\textcolor{red}{follow-up observation}

\textcolor{red}{observation 1}

\textcolor{red}{observation 2}

\textcolor{red}{sum-up from those two}


\section{Implications for research}

\textcolor{red}{how to improve scientific scene 1}

\textcolor{red}{how to improve scientific scene 2}

\textcolor{red}{how to improve scientific scene 3}


\section{Implications for software engineering professionals}

\textcolor{red}{how to improve professional scene 1}

\textcolor{red}{how to improve professional scene 2}

\textcolor{red}{how to improve professional scene 3}

\textcolor{red}{overall}


\section{Limitations and threats to validity}

\textcolor{red}{PUT THIS HERE OR THE SECTION BELOW Some things must be mentioned about the process of the first stage. First, the FSF outputted a ''license'' named ''other''. This ''license'' included at the time of observation 5282 known programs to FSF whose PCLs were not documented yet by the FSF. Although some of the programs had straightforward PCLs such as GPL-2.0-only we decided to leave these PCLs out of the scope of this thesis due to the large amount of the programs. The second note is about GNU's PCLs. Since we had the most trouble scraping the identifiers automatically from this website we decided to limit the PCLs only to ''Software Licenses'' as defined by the table of contents on the website. }

\textcolor{red}{SAME THING HERE approx duplicates were the result of going to the two listing websites that had the approaximately same looking licenses. then i just checked if they were actually some sort of duplicates of one another or if they already exist somewhere else. examples here. this is also a validity threat. problem with focusing software specific licenses is for example wtfpl. it is mostly used in software licensing but it doesn't quite clearly state that it is software specific license. maybe ill have to include the word "public license" and just include stuff that's not actually software specific or maybe ill make some exclusion criteria in order to get less non-software licenses}

\textcolor{red}{the order of wikipedia infobox was used for missing licenses so a validity threat and for the de-duplication in stage 2 and some other parts of the thesis you must declare. second stage licenses were fetched from scancode licensedb on 2025 mar 25 15:30}

\textcolor{red}{. dejavu and dbg-3.0 were also two other licenses that contained a space. this might indicate that the space is an accident that its simply just not found from a license listing site x. its also good to note that the python script was decided to be an valid approach since many of the licenses were actually found with the shortcode from the licensedb scancode. fetching 700 licenses by hand would have had time and validity issues. wayback machine could have been used to do the actual searching as well. this is unfortunately a validity issue but at least the source is available in wayback machine.}

\textcolor{red}{it seems like i have manually added licenses and invented shortcodes for them AT LEAST in DFSG license listing. FAL is a good example.}
\textcolor{orange}{many of the validity threats also are presented in 2.methods and 3 results like the fal free art license come upwith the shortcode from fsf and are not mentioned here again for redundant repetition.}

\textcolor{red}{For example licenses like CorkForkPL from FSF are just empty licenses. CorkForkPL is used however in MighTyD project but the license would have to be seen from a downloaded project or something like that. A new scope: only licenses that are one (1) click away from the initial license landing page can be copypasted to the manual licenses. just like the JahiaCSL has a URL on FSF for the license new location although the FSF page is empty.}

\textcolor{red}{licenses like MPL exist on FSF and GNU. it was not easily found from FSF (empty with links to programs using this) so it was gotten from GNU which was labeled as MPL1.1, which the FSF DOES have so i just boldly went with that. threat to validity.}

\textcolor{orange}{its good to note that systematic != automatic. our approach especially uses automation (python) to help the author use their human eye sight otherwise it would be more prone to error due to the large amount of licenses}

\textcolor{orange}{documenting missing licenses amount from their respective site was not possible since the missing licenses were just fetched in the wikipedia order so if SPDX was a hit we would never know if it was missing from the other site i.e. wrong shortcode or completely just missing. most of the missing licenses were by a feel of the author from fsf and gnu in that order. it was a good realization to invent that the goal of the thesis was not to fix the organization issues of the 5 listing sites but rather to, again, point out the madness of having so many open source licenses. same realization goes for the difficulity of making the automation tool}

\textcolor{orange}{\\"(source|software|program|code|module|public(s+)license|ware|(w+)ware)" was the first inc exc regex i used and it caught stuff like gfdl and thats how i ended up using exclusion only. note that documentation is not software but for example font is}

\textcolor{orange}{python script does not work on windows machine due to some os dependent path problems - validity threat}

\textcolor{red}{remember to document the validity L of human eye sight used majorly on third stage of search process. duplicate removal in tabs was done so that: i check the text if the n and n+1 and if they look pretty much the same i act and if the shortcodes look the sam i act.}

The major limitation of this study is that the subjective results could not be validated by multiple researchers. In a systematic review, it is standard practice and highly recommended to have at least two, if not more, individuals independently conduct the review processes and then cross validating the findings. This would result in the possibility of comparing individual exclusion decisions and other decicions, thereby increasing the credibility of the study. However, in this study, the methodology was thoroughly documented, which allows us to assert with confidence that the study has an appropriate level of of validity.

As a work of single researcher, there is also a chance of inaccuracy and bias in the literature selection and filtering process. As much of the literature had to be reviewed manually and then included/excluded on a qualitative basis, this is a known limitation and a threat to validity. Multiple rounds of documented filtering and a clear paper trail of all decisions made keeps this threat in the acceptable levels.

\subsection{Limitations of literature selection for review}
Efforts were made to ensure the inclusion of comprehensive set of literature in the search process. This was achieved by setting the starting point of PCL lists to the Wikipedia article of the MIT license.

However, as with all systematic literature reviews, a comprehensive manual review of all literature would have been a formidable task. Therefore, additional filtering was conducted. This filtering was carried out in two phases, starting with the application of inclusion/exclusion criteria, followed by a second phase focused on evaluating the nature of the PCLs and conducting a manual review. As a result of this second phase, a set of literature were excluded following a critical appraisal, with documentation and reasoning provided for each section.

The first phase of filtering has some notable limitations starting with the two PCL listing websites: SPDX and DFSG. Since the material was gathered to a spreadsheet program the duplicates were removed using the short identifier the listing page was using. Let's look at this validity threat using an example. Suppose our spreadsheet program has acquired the PCL with an identifier ''MIT''. The results of phase 1 will not include any other PCL marked with the identifier ''MIT''. In the worst case the identifier ''MIT'' could have actually been ''MIT-DFSG-edition'' but with the identifier of ''MIT''. Since there were so many PCLs in phase 1 it would not have been possible to check the uniqueness of all removed duplicates. One of the reasons why this would not have been feasible is that the listing sites would fetch the PCL contents from another webpage or at the second worst case, from another website. The worst case is that the URl is dead and we get HTTP 404. The amount of PCLs, duplicates and the lack of already existing tools makes this problem multilayered. However this is the integrity level we decided to live with.

FSF's PCL listing introduced us to pick another limitation for the scope of this thesis. The license shortcoded as ''other'' was not a PCL but instead a hyperlink to another listing webpage that listed programs that the FSF has no yet managed to document the license which the program uses. Although the one of the programs called ''babl'' was licensed as with ''gplv3'' the amount of undocumented programs was over 5200 at the time of observation. For this reason we are excluding the PCLs found indirectly from the category ''other''.

\textcolor{red}{tell about the validity threats of osi literature selection for review}

Lastly, GNU project's listing site allowed us to use a shortcut of sorts which we will document here for the purposes of acknowleding the limitations of it. The table of contents at the listing site marked certain consequtive PCLs as software PCLs. On top of this the PCLs were not organized into easily processable tables but rather in stacked on one another in rich text format. Although we decided to use regex on the HTMl file the included PCLs were only the ones that were simply under the header ''Software licenses''. In the worst case scenario GNU project could have misinterpreted some PCLs as non-software licenses thus making this thesis exclude them with a wrong reason. While from a quick glance and the existence of the other four PCL listing sites, we think it is still worth documenting when it comes to validity and the integrity of this thesis.

On top of too heavy filters we would also like to document the too light filters in the literature selection for review. We can see from \hyperref[appendix:a]{Appendix A} that for example PCLs with the literature identifiers L777 and L780 are almost the same regarding the shortcoded identifiers: ''ZPL - 2.1'' and ''ZPL-2.1''. The duplicate removal would have been seemingly simple to execute on phase 1. However with the presence of over 700 pieces of literature we decided not to give special treatment to any potential set of duplicates. While it is most possible that OSI's ''ZPL - 2.1'' is equivalent exactly to SPDX's ''ZPL-2.1'' we could not be sure without looking at their contents. This could have resulted duplicate PCLs in the literature selection for review but these type of duplicates are removed in phases 2 and 3 due to the PCLs being read in full.

As such we can note that the literature selection was done in a sufficient manner.

\subsection{Limitations in data extraction}

\textcolor{red}{importance of data extraction}

\textcolor{red}{lack of measurements and tooling}